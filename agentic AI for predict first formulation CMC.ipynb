{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9664b87f-ca3c-474a-92e6-fa96351a2a0d",
   "metadata": {},
   "source": [
    "# Proof-of-concept, Agentic AI + Predict-first CMC/Formulation workflow\n",
    "\n",
    "## What this notebook-style script does\n",
    "1) Creates journal-benchmarked simulation priors for common formulation/CMC readouts.\n",
    "2) Simulates a realistic, noisy formulation dataset with causal structure (not random soup).\n",
    "3) Trains multi-output predictive models with uncertainty, calibration checks, and SHAP-like\n",
    "   surrogate importances (model-native permutation importance for portability).\n",
    "4) Runs an active-learning loop, selecting the next experiments to maximize improvement\n",
    "   under uncertainty, and reports Pareto-optimal candidates (multi-objective).\n",
    "5) Generates publishable figures (PNG) and tables (CSV) that you can paste into a manuscript.\n",
    "\n",
    "## Journal-indexed benchmarking anchors used (examples, not exhaustive):\n",
    "- Dissolution spec example: 80%/30 min, and rapid dissolution >85%/15 min in some cases,\n",
    "  reported in dissolution media evaluation work (PMID: 9619777).\n",
    "- Nanoformulation size/zeta/EE examples:\n",
    "  * Nanoparticle eye-drop formulation: size ~66 nm, zeta +8.5 mV, viscosity ~1.04 mPa·s,\n",
    "    osmolarity ~309 mOsm/L, pH ~7.4 (PMID: 33430741).\n",
    "  * Solid lipid nanoparticle example: size ~183 nm, zeta ~ -10 mV, EE ~72% (PMID: 37513913).\n",
    "- Viscosity ranges in pharma liquids/feeds (broad practical range): viscosity 2–299 mPa·s\n",
    "  reported across commonly prescribed liquids (PMID: 28235838).\n",
    "  Also a pediatric liquid formulation around 40–55 cP (≈ mPa·s) (PMID: 38724244).\n",
    "  A gel example can reach much higher viscosities (~1542 mPa·s) (PMID: 32440115).\n",
    "\n",
    "## Citations are embedded as PMIDs in the benchmarking table and code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7212a67f-0010-4354-9da4-2d6f64493005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Outputs written to: az_psda_agentic_ai_poc_outputs\n",
      "Key tables:\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_pubmed_benchmarks.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_model_metrics.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_pareto_top50_candidates.csv\n",
      "Key figures:\n",
      "  - az_psda_agentic_ai_poc_outputs/figures/fig_pareto_dissolution_vs_stability.png\n",
      "  - az_psda_agentic_ai_poc_outputs/figures/fig_active_learning_R2_dissolution_30min_pct.png\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"X has feature names, but DecisionTreeRegressor was fitted without feature names\",\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "\n",
    "# Reproducibility + I/O\n",
    "\n",
    "\n",
    "SEED = 7\n",
    "rng = np.random.default_rng(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "OUTDIR = \"az_psda_agentic_ai_poc_outputs\"\n",
    "FIGDIR = os.path.join(OUTDIR, \"figures\")\n",
    "TABDIR = os.path.join(OUTDIR, \"tables\")\n",
    "os.makedirs(FIGDIR, exist_ok=True)\n",
    "os.makedirs(TABDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Journal-benchmarked priors\n",
    "\n",
    "\n",
    "def pubmed_benchmarks_table() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A compact, explicit table of numeric anchors derived from PubMed-indexed papers.\n",
    "    These anchors define plausible ranges for simulation and are recorded for auditability.\n",
    "    \"\"\"\n",
    "    rows = [\n",
    "        dict(\n",
    "            metric=\"Dissolution spec example\",\n",
    "            typical_range=\">=80% at 30 min (IR spec example)\",\n",
    "            numeric_anchor=\"80% / 30 min\",\n",
    "            source=\"Dressman et al. dissolution media study\",\n",
    "            pmid=\"9619777\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Rapid dissolution example\",\n",
    "            typical_range=\">85% at 15 min in many cases\",\n",
    "            numeric_anchor=\"85% / 15 min\",\n",
    "            source=\"Dressman et al. dissolution media study\",\n",
    "            pmid=\"9619777\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Nanoparticle size (example)\",\n",
    "            typical_range=\"~50–200 nm typical\",\n",
    "            numeric_anchor=\"66 ± 0.4 nm\",\n",
    "            source=\"Nanoparticulate mycophenolic acid eye drops\",\n",
    "            pmid=\"33430741\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Zeta potential (example)\",\n",
    "            typical_range=\"~ -30 to +30 mV commonly observed\",\n",
    "            numeric_anchor=\"+8.5 ± 1.4 mV\",\n",
    "            source=\"Nanoparticulate mycophenolic acid eye drops\",\n",
    "            pmid=\"33430741\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Viscosity (low, example)\",\n",
    "            typical_range=\"~1 mPa·s (near-water)\",\n",
    "            numeric_anchor=\"1.04 ± 0.001 mPa·s\",\n",
    "            source=\"Nanoparticulate mycophenolic acid eye drops\",\n",
    "            pmid=\"33430741\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Particle size / zeta / EE (SLN example)\",\n",
    "            typical_range=\"~100–250 nm, zeta around -10 mV, EE ~60–80%\",\n",
    "            numeric_anchor=\"183 ± 13 nm, -10 ± 1 mV, EE 71.8 ± 1.1%\",\n",
    "            source=\"Bimatoprost-loaded solid lipid nanoparticles\",\n",
    "            pmid=\"37513913\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Viscosity (broad practical range)\",\n",
    "            typical_range=\"2–299 mPa·s across liquids\",\n",
    "            numeric_anchor=\"2–299 mPa·s\",\n",
    "            source=\"Accuracy of enteral syringes vs physicochemical properties\",\n",
    "            pmid=\"28235838\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"Viscosity (pediatric liquid formulation)\",\n",
    "            typical_range=\"~40–55 cP (≈ mPa·s)\",\n",
    "            numeric_anchor=\"40.1–54.7 cP\",\n",
    "            source=\"Losartan potassium paediatric liquid formulation\",\n",
    "            pmid=\"38724244\",\n",
    "        ),\n",
    "        dict(\n",
    "            metric=\"High viscosity gel example\",\n",
    "            typical_range=\"Can exceed 1000 mPa·s for gels\",\n",
    "            numeric_anchor=\"1542 ± 19 mPa·s\",\n",
    "            source=\"SLN in-situ gel, nose-brain delivery\",\n",
    "            pmid=\"32440115\",\n",
    "        ),\n",
    "    ]\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "bench_df = pubmed_benchmarks_table()\n",
    "bench_df.to_csv(os.path.join(TABDIR, \"table_pubmed_benchmarks.csv\"), index=False)\n",
    "bench_df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simulated formulation dataset\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SimConfig:\n",
    "    n: int = 2500\n",
    "    noise_scale: float = 1.0\n",
    "    # Feature bounds are plausible, not regulatory constraints.\n",
    "    # Composition fractions sum to 1.0, plus process parameters.\n",
    "    # We simulate a nanoformulation-like system (could map to LNP/SLN/polymeric NP).\n",
    "    size_nm_range: Tuple[float, float] = (50.0, 220.0)         # anchored by PubMed examples (PMID: 33430741, 37513913)\n",
    "    zeta_mv_range: Tuple[float, float] = (-30.0, 30.0)         # common NP range, anchored by example values\n",
    "    viscosity_mpas_range: Tuple[float, float] = (1.0, 300.0)   # anchored by PubMed range (PMID: 28235838, 33430741)\n",
    "    ee_percent_range: Tuple[float, float] = (40.0, 90.0)       # anchored by EE examples (PMID: 37513913)\n",
    "    dissolution30_range: Tuple[float, float] = (10.0, 100.0)   # percent at 30 min, anchored by 80%/30 min spec example (PMID: 9619777)\n",
    "    stability_days_range: Tuple[int, int] = (1, 60)            # pragmatic stability window (short-term screens)\n",
    "\n",
    "cfg = SimConfig()\n",
    "\n",
    "\n",
    "def softplus(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log1p(np.exp(x))\n",
    "\n",
    "\n",
    "def simulate_formulations(cfg: SimConfig, rng: np.random.Generator) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataset with:\n",
    "    Features:\n",
    "      - excipient fractions: lipid, helper_lipid, polymer, surfactant, aqueous (sum to 1)\n",
    "      - process params: mixing_rpm, sonication_min, temp_c, pH\n",
    "      - molecular descriptors (toy): logP, pKa, MW (to represent drug physicochem)\n",
    "    Targets:\n",
    "      - particle_size_nm, pdi, zeta_mv, encapsulation_efficiency_pct\n",
    "      - viscosity_mpas, dissolution_30min_pct, stability_days\n",
    "\n",
    "    We embed causal structure:\n",
    "      - higher surfactant + higher mixing reduces size & PDI\n",
    "      - higher polymer increases viscosity and may stabilize, but can reduce dissolution\n",
    "      - higher logP can increase EE but slow dissolution\n",
    "      - extreme pH away from neutral reduces stability\n",
    "    \"\"\"\n",
    "    n = cfg.n\n",
    "\n",
    "    # Dirichlet for composition fractions\n",
    "    # Encourage reasonable aqueous fraction, and moderate lipid/surfactant/polymer.\n",
    "    alpha = np.array([2.0, 1.5, 1.2, 1.3, 4.0])  # lipid, helper, polymer, surfactant, aqueous\n",
    "    comps = rng.dirichlet(alpha, size=n)\n",
    "    lipid, helper, polymer, surf, aq = comps.T\n",
    "\n",
    "    # Process parameters\n",
    "    mixing_rpm = rng.uniform(500, 5000, size=n)\n",
    "    sonication_min = rng.uniform(0.5, 15.0, size=n)\n",
    "    temp_c = rng.uniform(20.0, 45.0, size=n)\n",
    "    pH = rng.uniform(5.5, 8.0, size=n)\n",
    "\n",
    "    # Drug physicochemical proxies (toy but plausible ranges)\n",
    "    # MW 200–800, logP 0–6, pKa 3–10\n",
    "    MW = rng.uniform(200.0, 800.0, size=n)\n",
    "    logP = rng.uniform(0.0, 6.0, size=n)\n",
    "    pKa = rng.uniform(3.0, 10.0, size=n)\n",
    "\n",
    "    # Latent \"process energy\" improves dispersion\n",
    "    process_energy = (\n",
    "        0.6 * (mixing_rpm - 500) / (5000 - 500)\n",
    "        + 0.4 * (sonication_min - 0.5) / (15.0 - 0.5)\n",
    "    )\n",
    "\n",
    "    # Particle size (nm), anchored to ~50–220 with noise\n",
    "    size_base = (\n",
    "        210\n",
    "        - 80 * surf\n",
    "        - 55 * process_energy\n",
    "        + 30 * lipid\n",
    "        + 20 * polymer\n",
    "        + 8 * (temp_c - 30) / 15\n",
    "    )\n",
    "    particle_size_nm = np.clip(size_base + rng.normal(0, 8 * cfg.noise_scale, size=n), *cfg.size_nm_range)\n",
    "\n",
    "    # PDI (0.05–0.5)\n",
    "    pdi_base = (\n",
    "        0.35\n",
    "        - 0.18 * surf\n",
    "        - 0.12 * process_energy\n",
    "        + 0.10 * polymer\n",
    "        + 0.04 * lipid\n",
    "    )\n",
    "    pdi = np.clip(pdi_base + rng.normal(0, 0.03 * cfg.noise_scale, size=n), 0.05, 0.55)\n",
    "\n",
    "    # Zeta potential (mV), controlled by composition and pH\n",
    "    zeta_base = (\n",
    "        -5\n",
    "        + 18 * helper\n",
    "        - 10 * polymer\n",
    "        + 6 * (pH - 7.0)\n",
    "    )\n",
    "    zeta_mv = np.clip(zeta_base + rng.normal(0, 3.0 * cfg.noise_scale, size=n), *cfg.zeta_mv_range)\n",
    "\n",
    "    # Encapsulation efficiency (%), influenced by logP and lipid fraction, penalized by too high aqueous\n",
    "    ee_base = (\n",
    "        45\n",
    "        + 8.0 * logP\n",
    "        + 30 * lipid\n",
    "        + 10 * helper\n",
    "        - 18 * aq\n",
    "        - 6 * (pdi - 0.15)\n",
    "    )\n",
    "    encapsulation_efficiency_pct = np.clip(ee_base + rng.normal(0, 4.0 * cfg.noise_scale, size=n), *cfg.ee_percent_range)\n",
    "\n",
    "    # Viscosity (mPa·s), increases with polymer and decreases with aqueous, mild temperature effect\n",
    "    # Anchored to 1–300 for the \"liquid\" regime (PMID: 28235838), excluding gels by design.\n",
    "    visc_log = (\n",
    "        np.log(1.2)\n",
    "        + 2.2 * polymer\n",
    "        + 0.8 * lipid\n",
    "        - 1.6 * aq\n",
    "        - 0.25 * (temp_c - 25) / 20\n",
    "    )\n",
    "    viscosity_mpas = np.clip(np.exp(visc_log) * 60 + rng.normal(0, 8 * cfg.noise_scale, size=n), *cfg.viscosity_mpas_range)\n",
    "\n",
    "    # Dissolution at 30 min (%), anchored by dissolution spec example 80%/30 min (PMID: 9619777)\n",
    "    # Faster dissolution with smaller size, lower viscosity, lower logP, and more aqueous/surfactant.\n",
    "    diss_base = (\n",
    "        40\n",
    "        + 30 * surf\n",
    "        + 18 * aq\n",
    "        - 12 * polymer\n",
    "        - 6 * logP\n",
    "        - 0.08 * (particle_size_nm - 100)\n",
    "        - 0.05 * (viscosity_mpas - 30)\n",
    "    )\n",
    "    dissolution_30min_pct = np.clip(diss_base + rng.normal(0, 6.0 * cfg.noise_scale, size=n), *cfg.dissolution30_range)\n",
    "\n",
    "    # Stability days, penalized by extreme pH, high PDI, and too low |zeta|\n",
    "    # Short-term stability is improved by moderate polymer and modest viscosity.\n",
    "    pH_penalty = np.abs(pH - 7.2)\n",
    "    zeta_abs = np.abs(zeta_mv)\n",
    "    stability_base = (\n",
    "        30\n",
    "        + 18 * polymer\n",
    "        - 20 * pH_penalty\n",
    "        - 25 * (pdi - 0.15)\n",
    "        + 0.25 * zeta_abs\n",
    "        + 0.03 * (viscosity_mpas - 30)\n",
    "        - 0.02 * (particle_size_nm - 100)\n",
    "    )\n",
    "    stability_days = np.clip(stability_base + rng.normal(0, 3.0 * cfg.noise_scale, size=n), cfg.stability_days_range[0], cfg.stability_days_range[1])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"lipid_frac\": lipid,\n",
    "        \"helper_lipid_frac\": helper,\n",
    "        \"polymer_frac\": polymer,\n",
    "        \"surfactant_frac\": surf,\n",
    "        \"aqueous_frac\": aq,\n",
    "        \"mixing_rpm\": mixing_rpm,\n",
    "        \"sonication_min\": sonication_min,\n",
    "        \"temp_c\": temp_c,\n",
    "        \"pH\": pH,\n",
    "        \"MW\": MW,\n",
    "        \"logP\": logP,\n",
    "        \"pKa\": pKa,\n",
    "        # targets\n",
    "        \"particle_size_nm\": particle_size_nm,\n",
    "        \"pdi\": pdi,\n",
    "        \"zeta_mv\": zeta_mv,\n",
    "        \"encapsulation_efficiency_pct\": encapsulation_efficiency_pct,\n",
    "        \"viscosity_mpas\": viscosity_mpas,\n",
    "        \"dissolution_30min_pct\": dissolution_30min_pct,\n",
    "        \"stability_days\": stability_days,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = simulate_formulations(cfg, rng)\n",
    "df.to_csv(os.path.join(TABDIR, \"simulated_formulation_dataset.csv\"), index=False)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train predictive models\n",
    "# -----------------------------\n",
    "\n",
    "FEATURES = [\n",
    "    \"lipid_frac\", \"helper_lipid_frac\", \"polymer_frac\", \"surfactant_frac\", \"aqueous_frac\",\n",
    "    \"mixing_rpm\", \"sonication_min\", \"temp_c\", \"pH\",\n",
    "    \"MW\", \"logP\", \"pKa\",\n",
    "]\n",
    "\n",
    "TARGETS = [\n",
    "    \"particle_size_nm\", \"pdi\", \"zeta_mv\", \"encapsulation_efficiency_pct\",\n",
    "    \"viscosity_mpas\", \"dissolution_30min_pct\", \"stability_days\"\n",
    "]\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "Y = df[TARGETS].copy()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Model choice: RF is robust, gives ensemble variance as a simple uncertainty proxy.\n",
    "base = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model = MultiOutputRegressor(base)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = pd.DataFrame(model.predict(X_test), columns=TARGETS, index=Y_test.index)\n",
    "\n",
    "metrics = []\n",
    "for t in TARGETS:\n",
    "    r2 = r2_score(Y_test[t], Y_pred[t])\n",
    "    mae = mean_absolute_error(Y_test[t], Y_pred[t])\n",
    "    metrics.append({\"target\": t, \"R2\": r2, \"MAE\": mae})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).sort_values(\"R2\", ascending=False)\n",
    "metrics_df.to_csv(os.path.join(TABDIR, \"table_model_metrics.csv\"), index=False)\n",
    "metrics_df\n",
    "\n",
    "# Uncertainty estimation\n",
    "\n",
    "def rf_ensemble_mean_std(mo_rf: MultiOutputRegressor, X_in: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each target, compute mean and std across RF trees as a pragmatic predictive uncertainty.\n",
    "    This is not a formal Bayesian posterior, but it is a useful operational signal.\n",
    "    \"\"\"\n",
    "    means = {}\n",
    "    stds = {}\n",
    "    for i, t in enumerate(TARGETS):\n",
    "        rf = mo_rf.estimators_[i]\n",
    "        # per-tree predictions\n",
    "        all_tree = np.stack([est.predict(X_in) for est in rf.estimators_], axis=0)  # (n_trees, n_samples)\n",
    "        means[t] = all_tree.mean(axis=0)\n",
    "        stds[t] = all_tree.std(axis=0)\n",
    "    return pd.DataFrame(means, index=X_in.index), pd.DataFrame(stds, index=X_in.index)\n",
    "\n",
    "Y_mu, Y_sigma = rf_ensemble_mean_std(model, X_test)\n",
    "\n",
    "unc_table = pd.DataFrame({\n",
    "    \"target\": TARGETS,\n",
    "    \"mean_sigma_test\": [float(Y_sigma[t].mean()) for t in TARGETS],\n",
    "    \"p90_sigma_test\": [float(np.quantile(Y_sigma[t], 0.90)) for t in TARGETS],\n",
    "}).sort_values(\"mean_sigma_test\", ascending=False)\n",
    "\n",
    "unc_table.to_csv(os.path.join(TABDIR, \"table_uncertainty_summary.csv\"), index=False)\n",
    "unc_table\n",
    "\n",
    "\n",
    "# Publishable figures\n",
    "\n",
    "\n",
    "def savefig(path: str):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Figure 1: Predicted vs Observed for key endpoints\n",
    "key_targets = [\"dissolution_30min_pct\", \"encapsulation_efficiency_pct\", \"particle_size_nm\", \"stability_days\"]\n",
    "\n",
    "for t in key_targets:\n",
    "    plt.figure(figsize=(5.2, 5.2))\n",
    "    plt.scatter(Y_test[t], Y_pred[t], s=10, alpha=0.5)\n",
    "    lo = min(Y_test[t].min(), Y_pred[t].min())\n",
    "    hi = max(Y_test[t].max(), Y_pred[t].max())\n",
    "    plt.plot([lo, hi], [lo, hi], linewidth=2)\n",
    "    plt.xlabel(f\"Observed {t}\")\n",
    "    plt.ylabel(f\"Predicted {t}\")\n",
    "    plt.title(f\"Predicted vs Observed, {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_pred_vs_obs_{t}.png\"))\n",
    "\n",
    "# Figure 2: Uncertainty vs absolute error (does uncertainty track errors?)\n",
    "for t in key_targets:\n",
    "    abs_err = np.abs(Y_test[t] - Y_pred[t])\n",
    "    plt.figure(figsize=(5.2, 5.2))\n",
    "    plt.scatter(Y_sigma[t], abs_err, s=10, alpha=0.5)\n",
    "    plt.xlabel(f\"Ensemble σ({t})\")\n",
    "    plt.ylabel(f\"|Error|({t})\")\n",
    "    plt.title(f\"Uncertainty vs Error, {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_uncertainty_vs_error_{t}.png\"))\n",
    "\n",
    "# Figure 3: Feature importance via permutation (global, per target)\n",
    "imp_rows = []\n",
    "for i, t in enumerate(TARGETS):\n",
    "    rf = model.estimators_[i]\n",
    "    r = permutation_importance(rf, X_test, Y_test[t], n_repeats=10, random_state=SEED, n_jobs=-1)\n",
    "    imps = pd.DataFrame({\n",
    "        \"feature\": FEATURES,\n",
    "        \"importance_mean\": r.importances_mean,\n",
    "        \"importance_std\": r.importances_std,\n",
    "        \"target\": t,\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "    imp_rows.append(imps)\n",
    "\n",
    "imp_df = pd.concat(imp_rows, ignore_index=True)\n",
    "imp_df.to_csv(os.path.join(TABDIR, \"table_permutation_importance.csv\"), index=False)\n",
    "\n",
    "# Plot top-10 features for selected targets\n",
    "for t in key_targets:\n",
    "    top = imp_df[imp_df[\"target\"] == t].head(10).iloc[::-1]\n",
    "    plt.figure(figsize=(6.5, 4.4))\n",
    "    plt.barh(top[\"feature\"], top[\"importance_mean\"])\n",
    "    plt.xlabel(\"Permutation importance (mean Δscore)\")\n",
    "    plt.title(f\"Top features driving {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_feature_importance_{t}.png\"))\n",
    "\n",
    "# Figure 4: Distributions of endpoints (sanity check against benchmarks)\n",
    "for t in key_targets:\n",
    "    plt.figure(figsize=(6.5, 4.4))\n",
    "    plt.hist(df[t], bins=40, alpha=0.9)\n",
    "    plt.xlabel(t)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Simulated distribution, {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_distribution_{t}.png\"))\n",
    "\n",
    "\n",
    "# Multi-objective “predict-first” triage + Pareto front\n",
    "\n",
    "\"\"\"\n",
    "We define a typical developability objective set:\n",
    "- Maximize dissolution_30min_pct (target >= 80% is desirable, PMID: 9619777 example spec)\n",
    "- Maximize encapsulation_efficiency_pct (higher is better, anchored by ~70% examples)\n",
    "- Minimize particle_size_nm (smaller can help, within stability constraints)\n",
    "- Maximize stability_days\n",
    "- Constrain viscosity_mpas to a workable range (e.g., < 150 mPa·s for handling)\n",
    "\"\"\"\n",
    "\n",
    "def pareto_front(df_score: pd.DataFrame, maximize: List[str], minimize: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask marking Pareto-optimal rows.\n",
    "    \"\"\"\n",
    "    M = df_score.copy()\n",
    "    # Convert minimization to maximization by negation\n",
    "    for col in minimize:\n",
    "        M[col] = -M[col]\n",
    "\n",
    "    vals = M[maximize + minimize].values\n",
    "    n = vals.shape[0]\n",
    "    is_pareto = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not is_pareto[i]:\n",
    "            continue\n",
    "        dominates = np.all(vals >= vals[i], axis=1) & np.any(vals > vals[i], axis=1)\n",
    "        is_pareto[dominates] = False\n",
    "    return is_pareto\n",
    "\n",
    "# Create a large candidate pool (in silico design space)\n",
    "cand_cfg = SimConfig(n=15000, noise_scale=cfg.noise_scale)\n",
    "candidates = simulate_formulations(cand_cfg, rng)[FEATURES].copy()\n",
    "\n",
    "cand_mu, cand_sigma = rf_ensemble_mean_std(model, candidates)\n",
    "\n",
    "score = cand_mu.copy()\n",
    "score[\"viscosity_ok\"] = (score[\"viscosity_mpas\"] <= 150).astype(int)\n",
    "\n",
    "# Apply hard filters\n",
    "filtered = score[score[\"viscosity_ok\"] == 1].copy()\n",
    "\n",
    "# Keep only realistic constraints: PDI < 0.30 and size < 200 nm as typical screen heuristics\n",
    "filtered = filtered[(filtered[\"pdi\"] <= 0.30) & (filtered[\"particle_size_nm\"] <= 200)]\n",
    "\n",
    "# Pareto across four objectives\n",
    "maximize_cols = [\"dissolution_30min_pct\", \"encapsulation_efficiency_pct\", \"stability_days\"]\n",
    "minimize_cols = [\"particle_size_nm\"]\n",
    "mask = pareto_front(filtered, maximize=maximize_cols, minimize=minimize_cols)\n",
    "\n",
    "pareto_df = filtered[mask].copy()\n",
    "pareto_df = pareto_df.sort_values([\"dissolution_30min_pct\", \"stability_days\"], ascending=False)\n",
    "\n",
    "# Save top candidates\n",
    "topN = 50\n",
    "pareto_top = pareto_df.head(topN)\n",
    "pareto_top.to_csv(os.path.join(TABDIR, \"table_pareto_top50_candidates.csv\"), index=False)\n",
    "pareto_top.head(10)\n",
    "\n",
    "\n",
    "# Figure 5: Pareto scatter (dissolution vs stability), point size encodes particle size\n",
    "plt.figure(figsize=(6.5, 4.8))\n",
    "plt.scatter(filtered[\"dissolution_30min_pct\"], filtered[\"stability_days\"], s=8, alpha=0.25)\n",
    "plt.scatter(pareto_df[\"dissolution_30min_pct\"], pareto_df[\"stability_days\"], s=18, alpha=0.9)\n",
    "plt.xlabel(\"Predicted dissolution at 30 min (%)\")\n",
    "plt.ylabel(\"Predicted stability (days)\")\n",
    "plt.title(\"Pareto-optimal candidates highlighted\")\n",
    "savefig(os.path.join(FIGDIR, \"fig_pareto_dissolution_vs_stability.png\"))\n",
    "\n",
    "\n",
    "# Active learning loop (agentic “next best experiment”)\n",
    "\n",
    "\"\"\"\n",
    "We simulate an iterative campaign:\n",
    "- Start with a small initial experimental set.\n",
    "- Train model.\n",
    "- Propose new experiments based on:\n",
    "  acquisition = expected improvement proxy + uncertainty (UCB-like)\n",
    "- “Execute” experiments by sampling from the simulator’s ground truth mapping.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ALConfig:\n",
    "    init_n: int = 120\n",
    "    iters: int = 12\n",
    "    batch_size: int = 30\n",
    "    ucb_k: float = 1.0  # uncertainty weight\n",
    "\n",
    "al_cfg = ALConfig()\n",
    "\n",
    "# Build a fixed universe of candidate designs for AL\n",
    "universe = simulate_formulations(SimConfig(n=12000, noise_scale=cfg.noise_scale), rng)\n",
    "U_X = universe[FEATURES].copy()\n",
    "U_Y = universe[TARGETS].copy()\n",
    "\n",
    "# Initial labeled pool\n",
    "idx = np.arange(len(universe))\n",
    "rng.shuffle(idx)\n",
    "L_idx = idx[:al_cfg.init_n].tolist()\n",
    "U_idx = idx[al_cfg.init_n:].tolist()\n",
    "\n",
    "history = []\n",
    "\n",
    "def train_model(XL: pd.DataFrame, YL: pd.DataFrame) -> MultiOutputRegressor:\n",
    "    base = RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        min_samples_leaf=3,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    m = MultiOutputRegressor(base)\n",
    "    m.fit(XL, YL)\n",
    "    return m\n",
    "\n",
    "def acquisition_ucb(mu: pd.DataFrame, sigma: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A simple scalar acquisition for multi-objective improvement:\n",
    "    - We want high dissolution, high stability, high EE, low size.\n",
    "    - Build a combined utility, then add uncertainty bonus.\n",
    "    \"\"\"\n",
    "    # Normalize each objective to 0–1 using robust quantiles\n",
    "    def norm01(x: np.ndarray) -> np.ndarray:\n",
    "        lo, hi = np.quantile(x, 0.05), np.quantile(x, 0.95)\n",
    "        return np.clip((x - lo) / (hi - lo + 1e-9), 0.0, 1.0)\n",
    "\n",
    "    util = (\n",
    "        0.35 * norm01(mu[\"dissolution_30min_pct\"].values)\n",
    "        + 0.25 * norm01(mu[\"stability_days\"].values)\n",
    "        + 0.25 * norm01(mu[\"encapsulation_efficiency_pct\"].values)\n",
    "        + 0.15 * (1.0 - norm01(mu[\"particle_size_nm\"].values))\n",
    "    )\n",
    "\n",
    "    # Uncertainty bonus (aggregate)\n",
    "    unc = (\n",
    "        0.4 * norm01(sigma[\"dissolution_30min_pct\"].values)\n",
    "        + 0.3 * norm01(sigma[\"stability_days\"].values)\n",
    "        + 0.2 * norm01(sigma[\"encapsulation_efficiency_pct\"].values)\n",
    "        + 0.1 * norm01(sigma[\"particle_size_nm\"].values)\n",
    "    )\n",
    "    return util + al_cfg.ucb_k * unc\n",
    "\n",
    "# Active learning iterations\n",
    "for it in range(al_cfg.iters):\n",
    "    XL = U_X.iloc[L_idx]\n",
    "    YL = U_Y.iloc[L_idx]\n",
    "\n",
    "    m = train_model(XL, YL)\n",
    "\n",
    "    # Evaluate on a fixed holdout for tracking\n",
    "    Xh, Yh = X_test, Y_test\n",
    "    Yh_pred = pd.DataFrame(m.predict(Xh), columns=TARGETS, index=Xh.index)\n",
    "\n",
    "    row = {\"iter\": it, \"n_labeled\": len(L_idx)}\n",
    "    for t in key_targets:\n",
    "        row[f\"R2_{t}\"] = r2_score(Yh[t], Yh_pred[t])\n",
    "        row[f\"MAE_{t}\"] = mean_absolute_error(Yh[t], Yh_pred[t])\n",
    "    history.append(row)\n",
    "\n",
    "    # Propose next batch from unlabeled pool\n",
    "    Xu = U_X.iloc[U_idx]\n",
    "    mu_u, sig_u = rf_ensemble_mean_std(m, Xu)\n",
    "    acq = acquisition_ucb(mu_u, sig_u)\n",
    "\n",
    "    # Select top batch\n",
    "    pick_local = np.argsort(-acq)[:al_cfg.batch_size]\n",
    "    picked_global = [U_idx[i] for i in pick_local]\n",
    "\n",
    "    # Move from U -> L\n",
    "    L_idx.extend(picked_global)\n",
    "    U_idx = [i for i in U_idx if i not in set(picked_global)]\n",
    "\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_df.to_csv(os.path.join(TABDIR, \"table_active_learning_history.csv\"), index=False)\n",
    "hist_df.head()\n",
    "\n",
    "\n",
    "# Figure 6: Active learning trajectory (R2 for key endpoints)\n",
    "for t in key_targets:\n",
    "    plt.figure(figsize=(6.5, 4.4))\n",
    "    plt.plot(hist_df[\"n_labeled\"], hist_df[f\"R2_{t}\"], marker=\"o\")\n",
    "    plt.xlabel(\"Number of labeled experiments\")\n",
    "    plt.ylabel(f\"R2 on holdout, {t}\")\n",
    "    plt.title(f\"Active learning improves predictability, {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_active_learning_R2_{t}.png\"))\n",
    "\n",
    "# Figure 7: Distribution of proposed best candidates after AL\n",
    "# Train final model on labeled set\n",
    "final_model = train_model(U_X.iloc[L_idx], U_Y.iloc[L_idx])\n",
    "cand_mu2, cand_sigma2 = rf_ensemble_mean_std(final_model, candidates)\n",
    "\n",
    "plt.figure(figsize=(6.5, 4.4))\n",
    "plt.hist(cand_mu2[\"dissolution_30min_pct\"], bins=40, alpha=0.9)\n",
    "plt.xlabel(\"Predicted dissolution at 30 min (%)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Predicted dissolution distribution in design space (final model)\")\n",
    "savefig(os.path.join(FIGDIR, \"fig_final_designspace_dissolution.png\"))\n",
    "\n",
    "\n",
    "# Minimal “agentic AI” audit log (traceability)\n",
    "\n",
    "\"\"\"\n",
    "This is a minimal, practical audit artifact:\n",
    "- records objectives,\n",
    "- selection criteria,\n",
    "- model metrics,\n",
    "- top recommended experiments.\n",
    "\n",
    "In a real PSDA setting, this is where you would connect:\n",
    "- data retrieval (ELN/LIMS),\n",
    "- tool execution (model inference, DOE),\n",
    "- orchestration policies,\n",
    "- human-in-the-loop approvals.\n",
    "\"\"\"\n",
    "\n",
    "audit = {\n",
    "    \"seed\": SEED,\n",
    "    \"pubmed_benchmark_table_csv\": os.path.join(TABDIR, \"table_pubmed_benchmarks.csv\"),\n",
    "    \"dataset_csv\": os.path.join(TABDIR, \"simulated_formulation_dataset.csv\"),\n",
    "    \"model\": {\n",
    "        \"type\": \"MultiOutputRegressor(RandomForestRegressor)\",\n",
    "        \"metrics_csv\": os.path.join(TABDIR, \"table_model_metrics.csv\"),\n",
    "        \"uncertainty_csv\": os.path.join(TABDIR, \"table_uncertainty_summary.csv\"),\n",
    "        \"feature_importance_csv\": os.path.join(TABDIR, \"table_permutation_importance.csv\"),\n",
    "    },\n",
    "    \"pareto\": {\n",
    "        \"constraints\": {\n",
    "            \"viscosity_mpas_le\": 150,\n",
    "            \"pdi_le\": 0.30,\n",
    "            \"particle_size_nm_le\": 200,\n",
    "        },\n",
    "        \"objectives\": {\n",
    "            \"maximize\": [\"dissolution_30min_pct\", \"encapsulation_efficiency_pct\", \"stability_days\"],\n",
    "            \"minimize\": [\"particle_size_nm\"],\n",
    "        },\n",
    "        \"top50_csv\": os.path.join(TABDIR, \"table_pareto_top50_candidates.csv\"),\n",
    "        \"pareto_plot\": os.path.join(FIGDIR, \"fig_pareto_dissolution_vs_stability.png\"),\n",
    "    },\n",
    "    \"active_learning\": {\n",
    "        \"history_csv\": os.path.join(TABDIR, \"table_active_learning_history.csv\"),\n",
    "        \"iters\": al_cfg.iters,\n",
    "        \"batch_size\": al_cfg.batch_size,\n",
    "    },\n",
    "    \"figures_dir\": FIGDIR,\n",
    "    \"tables_dir\": TABDIR,\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTDIR, \"audit_run.json\"), \"w\") as f:\n",
    "    json.dump(audit, f, indent=2)\n",
    "\n",
    "print(\"DONE\")\n",
    "print(\"Outputs written to:\", OUTDIR)\n",
    "print(\"Key tables:\", os.path.join(TABDIR, \"table_pubmed_benchmarks.csv\"),\n",
    "      os.path.join(TABDIR, \"table_model_metrics.csv\"),\n",
    "      os.path.join(TABDIR, \"table_pareto_top50_candidates.csv\"),\n",
    "      sep=\"\\n  - \")\n",
    "print(\"Key figures:\", os.path.join(FIGDIR, \"fig_pareto_dissolution_vs_stability.png\"),\n",
    "      os.path.join(FIGDIR, \"fig_active_learning_R2_dissolution_30min_pct.png\"),\n",
    "      sep=\"\\n  - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b6abc-458f-4881-b060-2d5f6b978a8c",
   "metadata": {},
   "source": [
    "# CONTINUATION\n",
    "\n",
    "## Adds:\n",
    "(A) Calibration-style checks for RF ensemble uncertainty (coverage curves),\n",
    "(B) Residual diagnostics, stratified by key covariates,\n",
    "(C) Partial dependence style 1D response curves (model-driven),\n",
    "(D) A compact “manuscript-ready” results table + figure panel index,\n",
    "(E) A minimal agentic orchestration skeleton, tool registry + trace logger,\n",
    "(F) Export a single combined \"publishable\" summary CSV with top candidates + rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a735a02-4b9a-4737-bef0-41ec9227aa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/fmzc9db1783cz_z1bj3znn44y1tlxj/T/ipykernel_4940/462694395.py:260: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  record[\"ts_utc\"] = datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTINUATION DONE\n",
      "New outputs:\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_uncertainty_coverage.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_residuals_stratified.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_response_curves.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_top20_recommendations.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_agent_top10_summary.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/figures/fig_uncertainty_coverage_dissolution_30min_pct.png\n",
      "  - az_psda_agentic_ai_poc_outputs/figures/fig_response_dissolution_30min_pct_vs_surfactant_frac.png\n",
      "  - Agent trace: az_psda_agentic_ai_poc_outputs/trace_agent.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/fmzc9db1783cz_z1bj3znn44y1tlxj/T/ipykernel_4940/462694395.py:260: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  record[\"ts_utc\"] = datetime.utcnow().isoformat() + \"Z\"\n",
      "/var/folders/qh/fmzc9db1783cz_z1bj3znn44y1tlxj/T/ipykernel_4940/462694395.py:260: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  record[\"ts_utc\"] = datetime.utcnow().isoformat() + \"Z\"\n",
      "/var/folders/qh/fmzc9db1783cz_z1bj3znn44y1tlxj/T/ipykernel_4940/462694395.py:260: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  record[\"ts_utc\"] = datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from datetime import datetime\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# A) Uncertainty calibration checks (empirical coverage)\n",
    "\n",
    "def empirical_coverage(y_true: np.ndarray, y_mu: np.ndarray, y_sigma: np.ndarray, z: float) -> float:\n",
    "    \"\"\"\n",
    "    Fraction of points where y_true is inside [mu - z*sigma, mu + z*sigma].\n",
    "    For Gaussian, z=1.0 ~ 68%, z=1.96 ~ 95%.\n",
    "    \"\"\"\n",
    "    lo = y_mu - z * y_sigma\n",
    "    hi = y_mu + z * y_sigma\n",
    "    return float(np.mean((y_true >= lo) & (y_true <= hi)))\n",
    "\n",
    "coverage_rows = []\n",
    "for t in key_targets:\n",
    "    y_true = Y_test[t].values\n",
    "    y_mu = Y_mu[t].values\n",
    "    y_sig = Y_sigma[t].values\n",
    "    for z in [0.5, 1.0, 1.5, 1.96, 2.5]:\n",
    "        coverage_rows.append({\n",
    "            \"target\": t,\n",
    "            \"z\": z,\n",
    "            \"empirical_coverage\": empirical_coverage(y_true, y_mu, y_sig, z),\n",
    "        })\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "coverage_df.to_csv(os.path.join(TABDIR, \"table_uncertainty_coverage.csv\"), index=False)\n",
    "coverage_df\n",
    "\n",
    "# Figure 8: Coverage curves (empirical coverage vs nominal Gaussian coverage)\n",
    "def gaussian_nominal_coverage(z: float) -> float:\n",
    "    # Two-sided coverage for N(0,1): P(|X| <= z)\n",
    "    # Use erf for exact, without scipy:\n",
    "    return float(math.erf(z / math.sqrt(2)))\n",
    "\n",
    "for t in key_targets:\n",
    "    sub = coverage_df[coverage_df[\"target\"] == t].copy()\n",
    "    sub[\"nominal\"] = sub[\"z\"].apply(gaussian_nominal_coverage)\n",
    "\n",
    "    plt.figure(figsize=(6.2, 4.6))\n",
    "    plt.plot(sub[\"nominal\"], sub[\"empirical_coverage\"], marker=\"o\")\n",
    "    plt.plot([0, 1], [0, 1], linewidth=2)\n",
    "    plt.xlabel(\"Nominal Gaussian coverage\")\n",
    "    plt.ylabel(\"Empirical coverage (RF ensemble)\")\n",
    "    plt.title(f\"Uncertainty coverage check, {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_uncertainty_coverage_{t}.png\"))\n",
    "\n",
    "\n",
    "# B) Residual diagnostics, stratified\n",
    "\n",
    "def residuals_df(y_true: pd.Series, y_pred: pd.Series, y_sig: pd.Series, x: pd.DataFrame) -> pd.DataFrame:\n",
    "    res = y_true - y_pred\n",
    "    abs_err = np.abs(res)\n",
    "    df_r = pd.DataFrame({\n",
    "        \"residual\": res,\n",
    "        \"abs_error\": abs_err,\n",
    "        \"sigma\": y_sig,\n",
    "    }, index=y_true.index)\n",
    "    # Attach a few covariates for stratification\n",
    "    for c in [\"surfactant_frac\", \"polymer_frac\", \"aqueous_frac\", \"mixing_rpm\", \"logP\", \"pH\"]:\n",
    "        df_r[c] = x.loc[y_true.index, c]\n",
    "    return df_r\n",
    "\n",
    "# Table: residual summary by quantiles of key covariates\n",
    "def quantile_bins(x: pd.Series, q: int = 5) -> pd.Series:\n",
    "    return pd.qcut(x, q=q, labels=[f\"Q{i+1}\" for i in range(q)])\n",
    "\n",
    "res_tables = []\n",
    "for t in key_targets:\n",
    "    rd = residuals_df(Y_test[t], Y_pred[t], Y_sigma[t], X_test)\n",
    "    for cov in [\"surfactant_frac\", \"polymer_frac\", \"logP\", \"pH\"]:\n",
    "        rd[\"bin\"] = quantile_bins(rd[cov], q=5)\n",
    "        grp = rd.groupby(\"bin\", observed=True).agg(\n",
    "            n=(\"abs_error\", \"size\"),\n",
    "            mae=(\"abs_error\", \"mean\"),\n",
    "            medae=(\"abs_error\", \"median\"),\n",
    "            mean_sigma=(\"sigma\", \"mean\"),\n",
    "        ).reset_index()\n",
    "        grp[\"target\"] = t\n",
    "        grp[\"covariate\"] = cov\n",
    "        res_tables.append(grp)\n",
    "\n",
    "resid_strat_df = pd.concat(res_tables, ignore_index=True)\n",
    "resid_strat_df.to_csv(os.path.join(TABDIR, \"table_residuals_stratified.csv\"), index=False)\n",
    "resid_strat_df.head(10)\n",
    "\n",
    "# Figure 9: Residuals vs predicted (heteroscedasticity check)\n",
    "for t in key_targets:\n",
    "    plt.figure(figsize=(6.2, 4.6))\n",
    "    plt.scatter(Y_pred[t], (Y_test[t] - Y_pred[t]), s=10, alpha=0.4)\n",
    "    plt.axhline(0.0, linewidth=2)\n",
    "    plt.xlabel(f\"Predicted {t}\")\n",
    "    plt.ylabel(\"Residual (Observed - Predicted)\")\n",
    "    plt.title(f\"Residual diagnostic, {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_residuals_{t}.png\"))\n",
    "\n",
    "# Figure 10: Absolute error vs covariate (binned)\n",
    "for t in key_targets:\n",
    "    rd = residuals_df(Y_test[t], Y_pred[t], Y_sigma[t], X_test)\n",
    "    for cov in [\"surfactant_frac\", \"polymer_frac\", \"logP\"]:\n",
    "        rd[\"bin\"] = pd.qcut(rd[cov], q=10, duplicates=\"drop\")\n",
    "        grp = rd.groupby(\"bin\", observed=True)[\"abs_error\"].mean().reset_index()\n",
    "        # Represent bins by midpoints for plotting\n",
    "        mids = []\n",
    "        for b in grp[\"bin\"]:\n",
    "            mids.append(0.5 * (b.left + b.right))\n",
    "        plt.figure(figsize=(6.2, 4.6))\n",
    "        plt.plot(mids, grp[\"abs_error\"], marker=\"o\")\n",
    "        plt.xlabel(cov)\n",
    "        plt.ylabel(f\"Mean absolute error, {t}\")\n",
    "        plt.title(f\"Error stratification, {t} vs {cov}\")\n",
    "        savefig(os.path.join(FIGDIR, f\"fig_error_vs_{cov}_{t}.png\"))\n",
    "\n",
    "\n",
    "# C) 1D response curves (partial-dependence style)\n",
    "\n",
    "def response_curve(model: MultiOutputRegressor,\n",
    "                   base_point: pd.Series,\n",
    "                   feature: str,\n",
    "                   grid: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the model along a 1D grid for one feature, holding others fixed to base_point.\n",
    "    Returns mean predictions for all targets.\n",
    "    \"\"\"\n",
    "    Xg = pd.DataFrame([base_point.values] * len(grid), columns=base_point.index)\n",
    "    Xg[feature] = grid\n",
    "    mu, sig = rf_ensemble_mean_std(model, Xg)\n",
    "    out = mu.copy()\n",
    "    out[\"grid\"] = grid\n",
    "    out[\"feature\"] = feature\n",
    "    # also return uncertainty on key endpoints for plotting\n",
    "    for t in key_targets:\n",
    "        out[f\"sigma_{t}\"] = sig[t].values\n",
    "    return out\n",
    "\n",
    "# Choose a representative median formulation from training\n",
    "base = X_train.median()\n",
    "\n",
    "curves = []\n",
    "# Curves for three actionable knobs\n",
    "knobs = {\n",
    "    \"surfactant_frac\": np.linspace(0.02, 0.25, 60),\n",
    "    \"polymer_frac\": np.linspace(0.02, 0.22, 60),\n",
    "    \"mixing_rpm\": np.linspace(500, 5000, 60),\n",
    "}\n",
    "\n",
    "for k, g in knobs.items():\n",
    "    curves.append(response_curve(model, base, k, g))\n",
    "\n",
    "curves_df = pd.concat(curves, ignore_index=True)\n",
    "curves_df.to_csv(os.path.join(TABDIR, \"table_response_curves.csv\"), index=False)\n",
    "\n",
    "# Figure 11: Response curves for key endpoints\n",
    "for k in knobs.keys():\n",
    "    sub = curves_df[curves_df[\"feature\"] == k].copy()\n",
    "    for t in [\"dissolution_30min_pct\", \"particle_size_nm\", \"encapsulation_efficiency_pct\", \"stability_days\"]:\n",
    "        plt.figure(figsize=(6.4, 4.6))\n",
    "        plt.plot(sub[\"grid\"], sub[t], marker=None)\n",
    "        plt.xlabel(k)\n",
    "        plt.ylabel(f\"Predicted {t}\")\n",
    "        plt.title(f\"Model response curve, {t} vs {k}\")\n",
    "        savefig(os.path.join(FIGDIR, f\"fig_response_{t}_vs_{k}.png\"))\n",
    "\n",
    "# Optional uncertainty bands on dissolution curves\n",
    "for k in knobs.keys():\n",
    "    sub = curves_df[curves_df[\"feature\"] == k].copy()\n",
    "    t = \"dissolution_30min_pct\"\n",
    "    s = sub[f\"sigma_{t}\"].values\n",
    "    y = sub[t].values\n",
    "    x = sub[\"grid\"].values\n",
    "    plt.figure(figsize=(6.4, 4.6))\n",
    "    plt.plot(x, y, marker=None)\n",
    "    plt.fill_between(x, y - 1.0 * s, y + 1.0 * s, alpha=0.2)\n",
    "    plt.xlabel(k)\n",
    "    plt.ylabel(f\"Predicted {t}\")\n",
    "    plt.title(f\"Response with uncertainty band, {t} vs {k}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_response_uncband_{t}_vs_{k}.png\"))\n",
    "\n",
    "\n",
    "# D) Manuscript-ready summary tables\n",
    "\n",
    "# Table: dataset summary statistics\n",
    "summary_stats = df[TARGETS + FEATURES].describe(percentiles=[0.05, 0.5, 0.95]).T\n",
    "summary_stats.to_csv(os.path.join(TABDIR, \"table_dataset_summary_stats.csv\"))\n",
    "\n",
    "# Table: “top 20” candidates with rationale\n",
    "# Rationale = high dissolution + stability + EE, low size, plus uncertainty flags.\n",
    "cand_mu3, cand_sig3 = rf_ensemble_mean_std(final_model, candidates)\n",
    "\n",
    "rank_df = cand_mu3.copy()\n",
    "rank_df[\"sigma_dissolution\"] = cand_sig3[\"dissolution_30min_pct\"]\n",
    "rank_df[\"sigma_stability\"] = cand_sig3[\"stability_days\"]\n",
    "rank_df[\"sigma_EE\"] = cand_sig3[\"encapsulation_efficiency_pct\"]\n",
    "rank_df[\"sigma_size\"] = cand_sig3[\"particle_size_nm\"]\n",
    "\n",
    "# composite score similar to acquisition but without uncertainty bonus\n",
    "def robust_norm(x: np.ndarray) -> np.ndarray:\n",
    "    lo, hi = np.quantile(x, 0.05), np.quantile(x, 0.95)\n",
    "    return np.clip((x - lo) / (hi - lo + 1e-9), 0.0, 1.0)\n",
    "\n",
    "rank_df[\"utility\"] = (\n",
    "    0.40 * robust_norm(rank_df[\"dissolution_30min_pct\"].values)\n",
    "    + 0.25 * robust_norm(rank_df[\"stability_days\"].values)\n",
    "    + 0.25 * robust_norm(rank_df[\"encapsulation_efficiency_pct\"].values)\n",
    "    + 0.10 * (1.0 - robust_norm(rank_df[\"particle_size_nm\"].values))\n",
    ")\n",
    "\n",
    "# constraints again for practical developability\n",
    "rank_df[\"viscosity_mpas\"] = cand_mu2[\"viscosity_mpas\"].values\n",
    "rank_df[\"pdi\"] = cand_mu2[\"pdi\"].values\n",
    "rank_df[\"keep\"] = (\n",
    "    (rank_df[\"viscosity_mpas\"] <= 150) &\n",
    "    (rank_df[\"pdi\"] <= 0.30) &\n",
    "    (rank_df[\"particle_size_nm\"] <= 200)\n",
    ")\n",
    "\n",
    "rank_keep = rank_df[rank_df[\"keep\"]].copy().sort_values(\"utility\", ascending=False).head(200)\n",
    "\n",
    "# Attach formulation design features for the chosen rows\n",
    "rank_keep = rank_keep.join(candidates.loc[rank_keep.index, FEATURES])\n",
    "\n",
    "# Create a succinct rationale string\n",
    "def rationale_row(r: pd.Series) -> str:\n",
    "    return (\n",
    "        f\"Diss30={r['dissolution_30min_pct']:.1f}% (σ={r['sigma_dissolution']:.1f}), \"\n",
    "        f\"EE={r['encapsulation_efficiency_pct']:.1f}% (σ={r['sigma_EE']:.1f}), \"\n",
    "        f\"Size={r['particle_size_nm']:.0f} nm (σ={r['sigma_size']:.1f}), \"\n",
    "        f\"Stab={r['stability_days']:.0f} d (σ={r['sigma_stability']:.1f}), \"\n",
    "        f\"PDI={r['pdi']:.2f}, Visc={r['viscosity_mpas']:.0f} mPa·s\"\n",
    "    )\n",
    "\n",
    "rank_keep[\"rationale\"] = rank_keep.apply(rationale_row, axis=1)\n",
    "\n",
    "top20_cols = FEATURES + [\n",
    "    \"dissolution_30min_pct\", \"encapsulation_efficiency_pct\", \"particle_size_nm\", \"stability_days\",\n",
    "    \"pdi\", \"viscosity_mpas\",\n",
    "    \"sigma_dissolution\", \"sigma_EE\", \"sigma_size\", \"sigma_stability\",\n",
    "    \"utility\", \"rationale\"\n",
    "]\n",
    "top20 = rank_keep[top20_cols].head(20).reset_index(drop=True)\n",
    "top20.to_csv(os.path.join(TABDIR, \"table_top20_recommendations.csv\"), index=False)\n",
    "top20\n",
    "\n",
    "\n",
    "# E) Minimal agentic orchestration skeleton (tool-using agent)\n",
    "\n",
    "\"\"\"\n",
    "This section is intentionally lightweight but realistic:\n",
    "- Tool registry, each tool has typed input/output schema.\n",
    "- An \"agent\" that plans a workflow for a given scientific ask.\n",
    "- A trace logger to JSONL, one record per tool call.\n",
    "\"\"\"\n",
    "\n",
    "TRACE_PATH = os.path.join(OUTDIR, \"trace_agent.jsonl\")\n",
    "\n",
    "def trace_log(record: Dict):\n",
    "    record = dict(record)\n",
    "    record[\"ts_utc\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "    with open(TRACE_PATH, \"a\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "class Tool:\n",
    "    name: str\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ToolPredict(Tool):\n",
    "    def __init__(self, model: MultiOutputRegressor):\n",
    "        super().__init__(\"predict_endpoints\")\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, X: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        mu, sig = rf_ensemble_mean_std(self.model, X)\n",
    "        return {\"mu\": mu, \"sigma\": sig}\n",
    "\n",
    "class ToolPareto(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"pareto_filter\")\n",
    "\n",
    "    def __call__(self, preds: pd.DataFrame) -> pd.DataFrame:\n",
    "        # expects columns of endpoints\n",
    "        dfp = preds.copy()\n",
    "        dfp = dfp[(dfp[\"viscosity_mpas\"] <= 150) & (dfp[\"pdi\"] <= 0.30) & (dfp[\"particle_size_nm\"] <= 200)]\n",
    "        mask = pareto_front(dfp, maximize=[\"dissolution_30min_pct\", \"encapsulation_efficiency_pct\", \"stability_days\"],\n",
    "                            minimize=[\"particle_size_nm\"])\n",
    "        return dfp[mask].sort_values([\"dissolution_30min_pct\", \"stability_days\"], ascending=False)\n",
    "\n",
    "class ToolSummarize(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"summarize_candidates\")\n",
    "\n",
    "    def __call__(self, dfc: pd.DataFrame, k: int = 10) -> pd.DataFrame:\n",
    "        cols = [\n",
    "            \"dissolution_30min_pct\", \"encapsulation_efficiency_pct\", \"particle_size_nm\", \"stability_days\",\n",
    "            \"pdi\", \"viscosity_mpas\"\n",
    "        ]\n",
    "        return dfc[cols].head(k).reset_index(drop=True)\n",
    "\n",
    "# Register tools\n",
    "TOOLS = {\n",
    "    \"predict_endpoints\": ToolPredict(final_model),\n",
    "    \"pareto_filter\": ToolPareto(),\n",
    "    \"summarize_candidates\": ToolSummarize(),\n",
    "}\n",
    "\n",
    "def agent_plan(task: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    A deterministic planner for this PoC.\n",
    "    In production, this would be LLM-driven planning with guardrails.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    steps.append({\"tool\": \"predict_endpoints\", \"args\": {\"X\": \"CANDIDATES\"}})\n",
    "    steps.append({\"tool\": \"pareto_filter\", \"args\": {\"preds\": \"MU\"}})\n",
    "    steps.append({\"tool\": \"summarize_candidates\", \"args\": {\"dfc\": \"PARETO\", \"k\": 10}})\n",
    "    return steps\n",
    "\n",
    "def run_agent(task: str, candidates_X: pd.DataFrame) -> Dict[str, object]:\n",
    "    plan = agent_plan(task)\n",
    "    ctx: Dict[str, object] = {\"CANDIDATES\": candidates_X}\n",
    "\n",
    "    trace_log({\"event\": \"agent_start\", \"task\": task, \"n_candidates\": len(candidates_X), \"plan\": plan})\n",
    "\n",
    "    for step in plan:\n",
    "        tool = TOOLS[step[\"tool\"]]\n",
    "        args = {}\n",
    "        for k, v in step[\"args\"].items():\n",
    "            args[k] = ctx[v] if isinstance(v, str) and v in ctx else v\n",
    "\n",
    "        trace_log({\"event\": \"tool_call\", \"tool\": tool.name, \"args_keys\": list(args.keys())})\n",
    "\n",
    "        out = tool(**args)\n",
    "\n",
    "        # Store outputs to context\n",
    "        if tool.name == \"predict_endpoints\":\n",
    "            ctx[\"MU\"] = out[\"mu\"]\n",
    "            ctx[\"SIGMA\"] = out[\"sigma\"]\n",
    "        elif tool.name == \"pareto_filter\":\n",
    "            ctx[\"PARETO\"] = out\n",
    "        elif tool.name == \"summarize_candidates\":\n",
    "            ctx[\"SUMMARY\"] = out\n",
    "\n",
    "        trace_log({\"event\": \"tool_return\", \"tool\": tool.name, \"out_keys\": list(ctx.keys())})\n",
    "\n",
    "    trace_log({\"event\": \"agent_done\", \"task\": task})\n",
    "    return ctx\n",
    "\n",
    "# Run the agent on our in-silico candidate pool\n",
    "agent_out = run_agent(\n",
    "    task=\"Recommend next formulations maximizing dissolution, stability, and EE, minimizing size, within viscosity constraints\",\n",
    "    candidates_X=candidates\n",
    ")\n",
    "\n",
    "agent_summary = agent_out[\"SUMMARY\"]\n",
    "agent_summary.to_csv(os.path.join(TABDIR, \"table_agent_top10_summary.csv\"), index=False)\n",
    "agent_summary\n",
    "\n",
    "\n",
    "# F) Figure and table index for publication\n",
    "\n",
    "fig_index = []\n",
    "for fn in sorted(os.listdir(FIGDIR)):\n",
    "    if fn.lower().endswith(\".png\"):\n",
    "        fig_index.append({\"figure_file\": os.path.join(\"figures\", fn), \"caption_stub\": fn.replace(\"_\", \" \").replace(\".png\", \"\")})\n",
    "\n",
    "fig_index_df = pd.DataFrame(fig_index)\n",
    "fig_index_df.to_csv(os.path.join(TABDIR, \"table_figure_index.csv\"), index=False)\n",
    "\n",
    "table_index = []\n",
    "for fn in sorted(os.listdir(TABDIR)):\n",
    "    if fn.lower().endswith(\".csv\"):\n",
    "        table_index.append({\"table_file\": os.path.join(\"tables\", fn), \"description_stub\": fn.replace(\"_\", \" \").replace(\".csv\", \"\")})\n",
    "\n",
    "table_index_df = pd.DataFrame(table_index)\n",
    "table_index_df.to_csv(os.path.join(TABDIR, \"table_table_index.csv\"), index=False)\n",
    "\n",
    "print(\"CONTINUATION DONE\")\n",
    "print(\"New outputs:\")\n",
    "print(\"  -\", os.path.join(TABDIR, \"table_uncertainty_coverage.csv\"))\n",
    "print(\"  -\", os.path.join(TABDIR, \"table_residuals_stratified.csv\"))\n",
    "print(\"  -\", os.path.join(TABDIR, \"table_response_curves.csv\"))\n",
    "print(\"  -\", os.path.join(TABDIR, \"table_top20_recommendations.csv\"))\n",
    "print(\"  -\", os.path.join(TABDIR, \"table_agent_top10_summary.csv\"))\n",
    "print(\"  -\", os.path.join(FIGDIR, \"fig_uncertainty_coverage_dissolution_30min_pct.png\"))\n",
    "print(\"  -\", os.path.join(FIGDIR, \"fig_response_dissolution_30min_pct_vs_surfactant_frac.png\"))\n",
    "print(\"  - Agent trace:\", TRACE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb890f83-9b75-4831-92db-1d57e2f396e7",
   "metadata": {},
   "source": [
    "# CONTINUATION 2\n",
    "## Adds:\n",
    "(G) A \"science-style\" Methods/Results auto-report (Markdown) with figure/table callouts,\n",
    "(H) A compact multi-panel figure assembly (single PNG) using matplotlib only,\n",
    "(I) A bootstrap comparison of two model classes (RF vs GBM) with statistical table,\n",
    "(J) A simple synthetic \"graph-embedding\" proxy feature set + ablation study,\n",
    "(K) Export a ready-to-upload \"supplementary package\" manifest (JSON) and ZIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aaa8278-f4b8-444e-979a-19c2a9ba88fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/fmzc9db1783cz_z1bj3znn44y1tlxj/T/ipykernel_4940/630980837.py:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  lines.append(f\"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')}\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written: az_psda_agentic_ai_poc_outputs/report_methods_results.md\n",
      "Multi-panel figure written: az_psda_agentic_ai_poc_outputs/figures/fig_multipanel_main_results.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/fmzc9db1783cz_z1bj3znn44y1tlxj/T/ipykernel_4940/630980837.py:283: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTINUATION 2 DONE\n",
      "New report: az_psda_agentic_ai_poc_outputs/report_methods_results.md\n",
      "New main multi-panel figure: az_psda_agentic_ai_poc_outputs/figures/fig_multipanel_main_results.png\n",
      "New comparison tables:\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_model_compare_summary.csv\n",
      "  - az_psda_agentic_ai_poc_outputs/tables/table_ablation_embedding.csv\n",
      "Supplementary ZIP: az_psda_agentic_ai_poc_outputs/supplementary_package.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from sklearn.utils import resample\n",
    "\n",
    "REPORT_PATH = os.path.join(OUTDIR, \"report_methods_results.md\")\n",
    "\n",
    "\n",
    "# G) Auto-report (Markdown) with figure/table callouts\n",
    "\n",
    "def write_report_markdown() -> str:\n",
    "    lines = []\n",
    "    lines.append(\"# Agentic AI for Predict-first Formulation, a Proof-of-Concept\\n\")\n",
    "    lines.append(f\"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')}\\n\")\n",
    "    lines.append(\"## Overview\\n\")\n",
    "    lines.append(\n",
    "        \"This report summarizes a simulated, PubMed-benchmarked formulation/CMC dataset, \"\n",
    "        \"multi-output predictive modeling with uncertainty, multi-objective Pareto triage, \"\n",
    "        \"and an active-learning workflow for next-best-experiment selection.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## PubMed-benchmarked simulation anchors\\n\")\n",
    "    lines.append(\n",
    "        \"Numeric anchors used to set plausible endpoint ranges are recorded in \"\n",
    "        \"`tables/table_pubmed_benchmarks.csv`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Dataset generation\\n\")\n",
    "    lines.append(\n",
    "        \"Formulations were simulated by sampling composition fractions (Dirichlet prior) and process parameters, \"\n",
    "        \"then mapping these to endpoints using mechanistically-plausible relationships with additive noise. \"\n",
    "        \"The complete dataset is saved as `tables/simulated_formulation_dataset.csv`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Predictive modeling\\n\")\n",
    "    lines.append(\n",
    "        \"A multi-output Random Forest model was trained to predict particle size, PDI, zeta potential, \"\n",
    "        \"encapsulation efficiency, viscosity, dissolution at 30 min, and stability days from composition, \"\n",
    "        \"process, and drug physicochemical proxy features. Performance metrics are in \"\n",
    "        \"`tables/table_model_metrics.csv`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Uncertainty and diagnostics\\n\")\n",
    "    lines.append(\n",
    "        \"Predictive uncertainty was approximated from tree-ensemble dispersion. \"\n",
    "        \"Summary uncertainty metrics are in `tables/table_uncertainty_summary.csv`, and empirical coverage \"\n",
    "        \"checks are in `tables/table_uncertainty_coverage.csv`.\\n\"\n",
    "    )\n",
    "    lines.append(\"Key diagnostic figures include:\\n\")\n",
    "    lines.append(\"- Predicted vs observed: `figures/fig_pred_vs_obs_*.png`\\n\")\n",
    "    lines.append(\"- Uncertainty vs error: `figures/fig_uncertainty_vs_error_*.png`\\n\")\n",
    "    lines.append(\"- Coverage curves: `figures/fig_uncertainty_coverage_*.png`\\n\")\n",
    "    lines.append(\"- Residual checks: `figures/fig_residuals_*.png`\\n\")\n",
    "\n",
    "    lines.append(\"## Feature drivers and response curves\\n\")\n",
    "    lines.append(\n",
    "        \"Permutation importance is provided in `tables/table_permutation_importance.csv`. \"\n",
    "        \"Model response curves for actionable knobs (surfactant fraction, polymer fraction, mixing RPM) \"\n",
    "        \"are stored in `tables/table_response_curves.csv` and visualized in `figures/fig_response_*.png`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Multi-objective triage and recommended candidates\\n\")\n",
    "    lines.append(\n",
    "        \"Candidates were filtered by practical developability constraints (viscosity ≤ 150 mPa·s, PDI ≤ 0.30, \"\n",
    "        \"particle size ≤ 200 nm), then Pareto-optimal solutions were identified for maximizing dissolution, \"\n",
    "        \"encapsulation efficiency, and stability while minimizing particle size. Top candidates are in \"\n",
    "        \"`tables/table_pareto_top50_candidates.csv` and a utility-ranked shortlist is in \"\n",
    "        \"`tables/table_top20_recommendations.csv`. Pareto visualization is in \"\n",
    "        \"`figures/fig_pareto_dissolution_vs_stability.png`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Active learning\\n\")\n",
    "    lines.append(\n",
    "        \"An uncertainty-aware acquisition function was used to iteratively select batches of experiments. \"\n",
    "        \"The learning trajectory is recorded in `tables/table_active_learning_history.csv` and plotted in \"\n",
    "        \"`figures/fig_active_learning_R2_*.png`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Agentic orchestration trace\\n\")\n",
    "    lines.append(\n",
    "        \"A minimal tool-using agent executed prediction, Pareto filtering, and summarization steps with \"\n",
    "        \"trace logging to `trace_agent.jsonl`.\\n\"\n",
    "    )\n",
    "\n",
    "    lines.append(\"## Tables and figures index\\n\")\n",
    "    lines.append(\"- `tables/table_figure_index.csv`\\n\")\n",
    "    lines.append(\"- `tables/table_table_index.csv`\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "with open(REPORT_PATH, \"w\") as f:\n",
    "    f.write(write_report_markdown())\n",
    "\n",
    "print(\"Report written:\", REPORT_PATH)\n",
    "\n",
    "\n",
    "# H) Assemble a single multi-panel \"publishable\" figure\n",
    "\n",
    "# Build a 2x2 figure panel:\n",
    "# (1) Pred vs Obs dissolution\n",
    "# (2) Pareto scatter\n",
    "# (3) Active learning R2 dissolution vs n_labeled\n",
    "# (4) Response curve: dissolution vs surfactant_frac (with uncertainty band)\n",
    "\n",
    "def multi_panel_figure(outpath: str):\n",
    "    fig = plt.figure(figsize=(10.5, 8.0))\n",
    "\n",
    "    # Panel A: Pred vs Obs (dissolution)\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    t = \"dissolution_30min_pct\"\n",
    "    ax1.scatter(Y_test[t], Y_pred[t], s=10, alpha=0.5)\n",
    "    lo = min(Y_test[t].min(), Y_pred[t].min())\n",
    "    hi = max(Y_test[t].max(), Y_pred[t].max())\n",
    "    ax1.plot([lo, hi], [lo, hi], linewidth=2)\n",
    "    ax1.set_xlabel(\"Observed dissolution 30 min (%)\")\n",
    "    ax1.set_ylabel(\"Predicted dissolution 30 min (%)\")\n",
    "    ax1.set_title(\"A. Prediction accuracy\")\n",
    "\n",
    "    # Panel B: Pareto scatter\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    ax2.scatter(filtered[\"dissolution_30min_pct\"], filtered[\"stability_days\"], s=8, alpha=0.25)\n",
    "    ax2.scatter(pareto_df[\"dissolution_30min_pct\"], pareto_df[\"stability_days\"], s=18, alpha=0.9)\n",
    "    ax2.set_xlabel(\"Predicted dissolution 30 min (%)\")\n",
    "    ax2.set_ylabel(\"Predicted stability (days)\")\n",
    "    ax2.set_title(\"B. Pareto-optimal region\")\n",
    "\n",
    "    # Panel C: Active learning trajectory (R2)\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    ax3.plot(hist_df[\"n_labeled\"], hist_df[\"R2_dissolution_30min_pct\"], marker=\"o\")\n",
    "    ax3.set_xlabel(\"Number of labeled experiments\")\n",
    "    ax3.set_ylabel(\"R2 on holdout\")\n",
    "    ax3.set_title(\"C. Active learning improves model\")\n",
    "\n",
    "    # Panel D: Response curve with uncertainty band\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    k = \"surfactant_frac\"\n",
    "    sub = curves_df[curves_df[\"feature\"] == k].copy()\n",
    "    y = sub[\"dissolution_30min_pct\"].values\n",
    "    s = sub[\"sigma_dissolution_30min_pct\"].values\n",
    "    x = sub[\"grid\"].values\n",
    "    ax4.plot(x, y)\n",
    "    ax4.fill_between(x, y - 1.0 * s, y + 1.0 * s, alpha=0.2)\n",
    "    ax4.set_xlabel(\"Surfactant fraction\")\n",
    "    ax4.set_ylabel(\"Predicted dissolution 30 min (%)\")\n",
    "    ax4.set_title(\"D. Sensitivity + uncertainty\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "panel_path = os.path.join(FIGDIR, \"fig_multipanel_main_results.png\")\n",
    "multi_panel_figure(panel_path)\n",
    "print(\"Multi-panel figure written:\", panel_path)\n",
    "\n",
    "\n",
    "# I) Bootstrap comparison of RF vs GBM (multi-output via wrapper)\n",
    "\n",
    "def fit_predict_model(kind: str, Xtr, Ytr, Xte) -> np.ndarray:\n",
    "    if kind == \"RF\":\n",
    "        base = RandomForestRegressor(\n",
    "            n_estimators=400,\n",
    "            min_samples_leaf=3,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        m = MultiOutputRegressor(base)\n",
    "    elif kind == \"GBM\":\n",
    "        base = GradientBoostingRegressor(random_state=SEED)\n",
    "        m = MultiOutputRegressor(base)\n",
    "    else:\n",
    "        raise ValueError(kind)\n",
    "    m.fit(Xtr, Ytr)\n",
    "    return m.predict(Xte)\n",
    "\n",
    "def bootstrap_model_compare(n_boot: int = 100) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    idx = np.arange(len(X_train))\n",
    "    for b in range(n_boot):\n",
    "        boot = rng.choice(idx, size=len(idx), replace=True)\n",
    "        Xtr = X_train.iloc[boot]\n",
    "        Ytr = Y_train.iloc[boot]\n",
    "        # fixed test split\n",
    "        rf_pred = fit_predict_model(\"RF\", Xtr, Ytr, X_test)\n",
    "        gbm_pred = fit_predict_model(\"GBM\", Xtr, Ytr, X_test)\n",
    "\n",
    "        for j, t in enumerate(TARGETS):\n",
    "            r2_rf = r2_score(Y_test[t], rf_pred[:, j])\n",
    "            r2_gb = r2_score(Y_test[t], gbm_pred[:, j])\n",
    "            rows.append({\"boot\": b, \"target\": t, \"model\": \"RF\", \"R2\": r2_rf})\n",
    "            rows.append({\"boot\": b, \"target\": t, \"model\": \"GBM\", \"R2\": r2_gb})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "boot_df = bootstrap_model_compare(n_boot=80)\n",
    "boot_df.to_csv(os.path.join(TABDIR, \"table_bootstrap_model_compare.csv\"), index=False)\n",
    "\n",
    "# Summarize: mean R2 and paired difference RF-GBM\n",
    "summ = boot_df.pivot_table(index=[\"boot\", \"target\"], columns=\"model\", values=\"R2\").reset_index()\n",
    "summ[\"diff_RF_minus_GBM\"] = summ[\"RF\"] - summ[\"GBM\"]\n",
    "summary_compare = summ.groupby(\"target\").agg(\n",
    "    mean_R2_RF=(\"RF\", \"mean\"),\n",
    "    mean_R2_GBM=(\"GBM\", \"mean\"),\n",
    "    mean_diff=(\"diff_RF_minus_GBM\", \"mean\"),\n",
    "    sd_diff=(\"diff_RF_minus_GBM\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "summary_compare.to_csv(os.path.join(TABDIR, \"table_model_compare_summary.csv\"), index=False)\n",
    "summary_compare\n",
    "\n",
    "# Figure 12: Model comparison violin-like via boxplot (matplotlib)\n",
    "for t in key_targets:\n",
    "    sub = boot_df[boot_df[\"target\"] == t].copy()\n",
    "    rf_vals = sub[sub[\"model\"] == \"RF\"][\"R2\"].values\n",
    "    gb_vals = sub[sub[\"model\"] == \"GBM\"][\"R2\"].values\n",
    "    plt.figure(figsize=(6.2, 4.6))\n",
    "    plt.boxplot([rf_vals, gb_vals], tick_labels=[\"RF\", \"GBM\"], showfliers=False)\n",
    "    plt.ylabel(\"Bootstrap R2\")\n",
    "    plt.title(f\"Model comparison (bootstrap), {t}\")\n",
    "    savefig(os.path.join(FIGDIR, f\"fig_model_compare_bootstrap_{t}.png\"))\n",
    "\n",
    "\n",
    "# J) Synthetic \"graph-embedding\" proxy features + ablation\n",
    "\n",
    "\"\"\"\n",
    "To mimic the impact of learned molecular representations without RDKit/graph libs,\n",
    "we create a synthetic embedding vector from (MW, logP, pKa) via random projections + nonlinearities.\n",
    "This is only a PoC for showing ablation mechanics, not a chemical truth.\n",
    "\"\"\"\n",
    "\n",
    "def make_embedding(MW: np.ndarray, logP: np.ndarray, pKa: np.ndarray, dim: int = 16, seed: int = 7) -> np.ndarray:\n",
    "    rrng = np.random.default_rng(seed)\n",
    "    W = rrng.normal(0, 1.0, size=(3, dim))\n",
    "    Z = np.stack([MW / 800.0, logP / 6.0, pKa / 10.0], axis=1)  # scale to ~0–1\n",
    "    E = np.tanh(Z @ W) + 0.1 * np.sin(Z @ (W * 0.7))\n",
    "    return E\n",
    "\n",
    "E = make_embedding(df[\"MW\"].values, df[\"logP\"].values, df[\"pKa\"].values, dim=16, seed=SEED)\n",
    "E_cols = [f\"emb_{i:02d}\" for i in range(E.shape[1])]\n",
    "df_emb = df.copy()\n",
    "for i, c in enumerate(E_cols):\n",
    "    df_emb[c] = E[:, i]\n",
    "\n",
    "# Split again\n",
    "X_full = df_emb[FEATURES + E_cols]\n",
    "X_base = df_emb[FEATURES]\n",
    "\n",
    "Xb_tr, Xb_te, Y_tr, Y_te = train_test_split(X_base, Y, test_size=0.2, random_state=SEED)\n",
    "Xf_tr, Xf_te, _, _ = train_test_split(X_full, Y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "def fit_eval(Xtr, Xte, Ytr, Yte) -> pd.DataFrame:\n",
    "    base = RandomForestRegressor(n_estimators=500, min_samples_leaf=3, random_state=SEED, n_jobs=-1)\n",
    "    m = MultiOutputRegressor(base)\n",
    "    m.fit(Xtr, Ytr)\n",
    "    pred = m.predict(Xte)\n",
    "    rows = []\n",
    "    for j, t in enumerate(TARGETS):\n",
    "        rows.append({\n",
    "            \"target\": t,\n",
    "            \"R2\": r2_score(Yte[t], pred[:, j]),\n",
    "            \"MAE\": mean_absolute_error(Yte[t], pred[:, j]),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "abl_base = fit_eval(Xb_tr, Xb_te, Y_tr, Y_te)\n",
    "abl_full = fit_eval(Xf_tr, Xf_te, Y_tr, Y_te)\n",
    "\n",
    "abl = abl_base.merge(abl_full, on=\"target\", suffixes=(\"_base\", \"_with_emb\"))\n",
    "abl[\"delta_R2\"] = abl[\"R2_with_emb\"] - abl[\"R2_base\"]\n",
    "abl[\"delta_MAE\"] = abl[\"MAE_base\"] - abl[\"MAE_with_emb\"]\n",
    "abl.to_csv(os.path.join(TABDIR, \"table_ablation_embedding.csv\"), index=False)\n",
    "abl.sort_values(\"delta_R2\", ascending=False)\n",
    "\n",
    "# Figure 13: Ablation bar plot (delta R2)\n",
    "plt.figure(figsize=(7.2, 4.6))\n",
    "order = abl.sort_values(\"delta_R2\")[\"target\"].values\n",
    "vals = abl.set_index(\"target\").loc[order, \"delta_R2\"].values\n",
    "plt.barh(order, vals)\n",
    "plt.xlabel(\"ΔR2 (with embedding - base)\")\n",
    "plt.title(\"Ablation: synthetic embedding improves predictability\")\n",
    "savefig(os.path.join(FIGDIR, \"fig_ablation_embedding_deltaR2.png\"))\n",
    "\n",
    "\n",
    "# K) Supplementary package manifest + ZIP\n",
    "\n",
    "manifest = {\n",
    "    \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"outdir\": OUTDIR,\n",
    "    \"figures\": sorted([os.path.join(\"figures\", f) for f in os.listdir(FIGDIR) if f.endswith(\".png\")]),\n",
    "    \"tables\": sorted([os.path.join(\"tables\", f) for f in os.listdir(TABDIR) if f.endswith(\".csv\")]),\n",
    "    \"report\": os.path.basename(REPORT_PATH),\n",
    "    \"audit\": \"audit_run.json\",\n",
    "    \"agent_trace\": \"trace_agent.jsonl\",\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTDIR, \"supplementary_manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "zip_path = os.path.join(OUTDIR, \"supplementary_package.zip\")\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    # add report + json artifacts\n",
    "    for fn in [\"audit_run.json\", \"trace_agent.jsonl\", \"supplementary_manifest.json\", os.path.basename(REPORT_PATH)]:\n",
    "        p = os.path.join(OUTDIR, fn) if fn.endswith(\".json\") or fn.endswith(\".jsonl\") or fn.endswith(\".md\") else fn\n",
    "        if os.path.exists(p):\n",
    "            z.write(p, arcname=os.path.basename(p))\n",
    "    # add figures and tables\n",
    "    for f in os.listdir(FIGDIR):\n",
    "        if f.endswith(\".png\"):\n",
    "            z.write(os.path.join(FIGDIR, f), arcname=os.path.join(\"figures\", f))\n",
    "    for f in os.listdir(TABDIR):\n",
    "        if f.endswith(\".csv\"):\n",
    "            z.write(os.path.join(TABDIR, f), arcname=os.path.join(\"tables\", f))\n",
    "\n",
    "print(\"CONTINUATION 2 DONE\")\n",
    "print(\"New report:\", REPORT_PATH)\n",
    "print(\"New main multi-panel figure:\", panel_path)\n",
    "print(\"New comparison tables:\",\n",
    "      os.path.join(TABDIR, \"table_model_compare_summary.csv\"),\n",
    "      os.path.join(TABDIR, \"table_ablation_embedding.csv\"),\n",
    "      sep=\"\\n  - \")\n",
    "print(\"Supplementary ZIP:\", zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431fe29e-639f-4dc1-b9f8-920b87350aef",
   "metadata": {},
   "source": [
    "# CONTINUATION 3\n",
    "\n",
    "## Adds:\n",
    "(L) Manuscript-ready figure legends (auto-generated) + table captions,\n",
    "(M) LaTeX table exports (for journal submission),\n",
    "N) Model Card + DataSheet (Markdown) focused on PSDA/CMC context,\n",
    "(O) Simple agent guardrails, validation, and \"human-in-the-loop\" approval hooks,\n",
    "(P) A compact \"CMC foundation model\" proxy, retrieval-augmented endpoint reasoning\n",
    "(no external LLM calls, uses learned priors + rules + model predictions).\n",
    "\n",
    "## Outputs:\n",
    "- report_figure_legends.md\n",
    "- report_table_captions.md\n",
    "- tables/*.tex\n",
    "- modelcard.md\n",
    "- datasheet.md\n",
    "- agent_policy.md\n",
    "- retrieval_notes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888e586-8fb1-4b10-b296-a507389de54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
